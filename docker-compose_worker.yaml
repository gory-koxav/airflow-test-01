# =============================================================================
# Worker Base Configuration (Airflow Distributed Architecture)
# =============================================================================
#
# FILE STRUCTURE (layered compose files):
# ┌─────────────────────────────────────────────────────────────────────────┐
# │ docker-compose_worker.yaml      <- THIS FILE (base: services, volumes)  │
# ├─────────────────────────────────────────────────────────────────────────┤
# │ docker-compose_worker.dev.yaml  <- DEV:  external network (same host)   │
# │ docker-compose_worker.prod.yaml <- PROD: bridge network + extra_hosts   │
# ├─────────────────────────────────────────────────────────────────────────┤
# │ docker-compose_worker.gpu.yaml  <- GPU:  NVIDIA GPU (if WORKER_GPU=true)│
# └─────────────────────────────────────────────────────────────────────────┘
#
# USAGE (handled by start-workers.sh):
#   ./start-workers.sh dev          # base + dev
#   ./start-workers.sh prod         # base + prod
#   (GPU added automatically when WORKER_GPU=true in worker env file)
#
# WORKER ENV FILES (env/{dev,prod}/worker-{type}-{bay}.env):
#   - worker-obs-64bay.env   (Observer, GPU enabled)
#   - worker-proc-64bay.env  (Processor, no GPU)
# =============================================================================

---
x-airflow-worker:
  &airflow-worker
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.1.3}
  # env_file is passed via command line: --env-file env/base.env --env-file env/{dev,prod}/worker.env
  environment:
    &airflow-worker-env
    # Core configuration
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    # Execution API Server URL - Required for Airflow 3.x workers
    # [DEVELOPMENT] Workers connect to apiserver via Docker network
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow-apiserver:8080/execution/"
    # [PRODUCTION-CHANGE] Use main server IP when deploying to separate servers
    # AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://${MAIN_SERVER_IP}:8080/execution/"

    # Database connection to main server
    # [DEVELOPMENT] Using container hostnames when on same machine
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD:-airflow}@${POSTGRES_HOST}:${POSTGRES_PORT:-5432}/airflow
    # [PRODUCTION-CHANGE] Uncomment below and comment above when deploying to separate servers
    # AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD:-airflow}@${MAIN_SERVER_IP}:${POSTGRES_PORT:-5432}/airflow

    # Celery configuration
    # [DEVELOPMENT] Using container hostnames when on same machine
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD:-airflow}@${POSTGRES_HOST}:${POSTGRES_PORT:-5432}/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD:-}@${REDIS_HOST}:${REDIS_PORT:-6379}/0
    # [PRODUCTION-CHANGE] Uncomment below and comment above when deploying to separate servers
    # AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD:-airflow}@${MAIN_SERVER_IP}:${POSTGRES_PORT:-5432}/airflow
    # AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD:-}@${MAIN_SERVER_IP}:${REDIS_PORT:-6379}/0

    # Worker specific configuration
    AIRFLOW__CELERY__WORKER_CONCURRENCY: ${WORKER_CONCURRENCY:-8}
    AIRFLOW__CELERY__WORKER_AUTOSCALE: ${WORKER_AUTOSCALE:-16,8}
    AIRFLOW__CELERY__WORKER_MAX_TASKS_PER_CHILD: ${WORKER_MAX_TASKS_PER_CHILD:-100}
    AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER: ${WORKER_PREFETCH_MULTIPLIER:-1}

    # Security
    AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}

    # Logging
    AIRFLOW__LOGGING__REMOTE_LOGGING: ${REMOTE_LOGGING:-false}
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: ${REMOTE_LOG_FOLDER:-s3://airflow-logs}
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: ${REMOTE_LOG_CONN_ID:-aws_default}

    # Required for proper Celery worker shutdown
    DUMB_INIT_SETSID: "0"

    # Worker identification
    WORKER_NAME: ${WORKER_NAME:-worker-1}
    WORKER_CLASS: ${WORKER_CLASS:-sync}

    # Additional pip requirements if needed
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}

    # Custom config file path
    AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'

    # Note: Dependencies are installed directly into Airflow's Python environment
    # via "uv pip install --system" in Dockerfile, so no venv PATH override needed

  volumes:
    # DAGs volume - can be mounted via NFS, synced via Git, or copied manually
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags:ro  # Read-only for workers
    # Logs volume - should be writable
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    # Plugins volume - read-only for workers
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins:ro
    # Config volume - read-only for workers
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config:ro
    # Source code volume - read-only for workers
    - ${AIRFLOW_PROJ_DIR:-.}/src:/opt/airflow/src:ro
    # Shared storage for observation/fusion/tracking data
    # DEV: HOST_DATA_PATH=./data (local project folder)
    # PROD: HOST_DATA_PATH=/mnt/airflow-data (NFS mount)
    # Workers need read-write access to shared storage
    - ${HOST_DATA_PATH:-./data}:/opt/airflow/data

  user: "${AIRFLOW_UID:-50000}:0"

  # Resource limits
  deploy:
    resources:
      limits:
        cpus: '${WORKER_CPU_LIMIT:-4}'
        memory: ${WORKER_MEMORY_LIMIT:-8G}
      reservations:
        cpus: '${WORKER_CPU_RESERVATION:-2}'
        memory: ${WORKER_MEMORY_RESERVATION:-4G}

services:
  # Airflow Worker Service
  airflow-worker:
    <<: *airflow-worker
    # Set hostname to WORKER_NAME for better identification in Flower
    hostname: ${WORKER_NAME}
    # Airflow 3.x uses 'airflow celery worker' command
    # --queues: specifies which queue(s) this worker will consume
    # --hostname: helps identify workers in monitoring tools (Flower)
    # [IMPORTANT] WORKER_QUEUES determines which tasks this worker handles
    #   - Observer workers: obs_{bay_id} (e.g., obs_3bay_north)
    #   - Processor workers: proc_{bay_id} (e.g., proc_3bay_north)
    command: celery worker --queues ${WORKER_QUEUES:-default}
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    networks:
      - worker-network
    # [PRODUCTION-CHANGE] Uncomment extra_hosts when deploying to separate servers
    # extra_hosts:
    #   - "main-server:${MAIN_SERVER_IP}"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Optional: Worker-specific Flower for monitoring
  flower-worker:
    <<: *airflow-worker
    command: celery flower --broker=${REDIS_URL:-redis://:${REDIS_PASSWORD:-}@${MAIN_SERVER_IP}:${REDIS_PORT:-6379}/0}
    profiles:
      - flower
    ports:
      - "${FLOWER_PORT:-5555}:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    networks:
      - worker-network
    extra_hosts:
      - "main-server:${MAIN_SERVER_IP}"

  # Optional: DAG Sync Service (using Git)
  dag-sync:
    image: alpine/git:latest
    profiles:
      - git-sync
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags:/dags
    environment:
      GIT_REPO: ${DAG_GIT_REPO}
      GIT_BRANCH: ${DAG_GIT_BRANCH:-main}
      GIT_SYNC_INTERVAL: ${DAG_SYNC_INTERVAL:-60}
    entrypoint: /bin/sh
    command:
      - -c
      - |
        while true; do
          echo "Syncing DAGs from git repository..."
          if [ ! -d /dags/.git ]; then
            git clone --branch $${GIT_BRANCH} $${GIT_REPO} /tmp/repo
            cp -r /tmp/repo/* /dags/
            rm -rf /tmp/repo
          else
            cd /dags && git pull origin $${GIT_BRANCH}
          fi
          echo "DAG sync complete. Waiting $${GIT_SYNC_INTERVAL} seconds..."
          sleep $${GIT_SYNC_INTERVAL}
        done
    restart: always
    networks:
      - worker-network

  # Optional: Log Shipper (Filebeat)
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    profiles:
      - logging
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs:ro
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      ELASTICSEARCH_HOSTS: ${ELASTICSEARCH_HOSTS:-http://${MAIN_SERVER_IP}:9200}
      WORKER_ID: ${WORKER_NAME:-worker-1}
    user: root
    restart: always
    networks:
      - worker-network
    extra_hosts:
      - "elasticsearch:${MAIN_SERVER_IP}"

  # Optional: Monitoring Agent (Node Exporter for Prometheus)
  node-exporter:
    image: prom/node-exporter:latest
    profiles:
      - monitoring
    ports:
      - "${NODE_EXPORTER_PORT:-9100}:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+)($$|/)'
    restart: always
    networks:
      - worker-network

  # Optional: Container Monitoring (cAdvisor)
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    profiles:
      - monitoring
    ports:
      - "${CADVISOR_PORT:-8080}:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    privileged: true
    restart: always
    networks:
      - worker-network

networks:
  worker-network:
    # Network configuration is defined in override files:
    # - DEV:  docker-compose_worker.dev.yaml  (external network, same machine)
    # - PROD: docker-compose_worker.prod.yaml (bridge network, separate machines)
    driver: bridge