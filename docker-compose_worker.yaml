# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Worker Server Configuration for Distributed Airflow Architecture
# This configuration runs only the Airflow Worker component
# It connects to the main server's PostgreSQL and Redis instances

---
x-airflow-worker:
  &airflow-worker
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.1.3}
  # env_file is passed via command line: --env-file env/base.env --env-file env/{dev,prod}/worker.env
  environment:
    &airflow-worker-env
    # Core configuration
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

    # Database connection to main server
    # [DEVELOPMENT] Using container hostnames when on same machine
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD:-airflow}@${POSTGRES_HOST}:${POSTGRES_PORT:-5432}/airflow
    # [PRODUCTION-CHANGE] Uncomment below and comment above when deploying to separate servers
    # AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD:-airflow}@${MAIN_SERVER_IP}:${POSTGRES_PORT:-5432}/airflow

    # Celery configuration
    # [DEVELOPMENT] Using container hostnames when on same machine
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD:-airflow}@${POSTGRES_HOST}:${POSTGRES_PORT:-5432}/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD:-}@${REDIS_HOST}:${REDIS_PORT:-6379}/0
    # [PRODUCTION-CHANGE] Uncomment below and comment above when deploying to separate servers
    # AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD:-airflow}@${MAIN_SERVER_IP}:${POSTGRES_PORT:-5432}/airflow
    # AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD:-}@${MAIN_SERVER_IP}:${REDIS_PORT:-6379}/0

    # Worker specific configuration
    AIRFLOW__CELERY__WORKER_CONCURRENCY: ${WORKER_CONCURRENCY:-8}
    AIRFLOW__CELERY__WORKER_AUTOSCALE: ${WORKER_AUTOSCALE:-16,8}
    AIRFLOW__CELERY__WORKER_MAX_TASKS_PER_CHILD: ${WORKER_MAX_TASKS_PER_CHILD:-100}
    AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER: ${WORKER_PREFETCH_MULTIPLIER:-1}

    # Security
    AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}

    # Logging
    AIRFLOW__LOGGING__REMOTE_LOGGING: ${REMOTE_LOGGING:-false}
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: ${REMOTE_LOG_FOLDER:-s3://airflow-logs}
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: ${REMOTE_LOG_CONN_ID:-aws_default}

    # Required for proper Celery worker shutdown
    DUMB_INIT_SETSID: "0"

    # Worker identification
    WORKER_NAME: ${WORKER_NAME:-worker-1}
    WORKER_CLASS: ${WORKER_CLASS:-sync}

    # Additional pip requirements if needed
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}

    # Custom config file path
    AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'

  volumes:
    # DAGs volume - can be mounted via NFS, synced via Git, or copied manually
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags:ro  # Read-only for workers
    # Logs volume - should be writable
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    # Plugins volume - read-only for workers
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins:ro
    # Config volume - read-only for workers
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config:ro

  user: "${AIRFLOW_UID:-50000}:0"

  # Resource limits
  deploy:
    resources:
      limits:
        cpus: '${WORKER_CPU_LIMIT:-4}'
        memory: ${WORKER_MEMORY_LIMIT:-8G}
      reservations:
        cpus: '${WORKER_CPU_RESERVATION:-2}'
        memory: ${WORKER_MEMORY_RESERVATION:-4G}

services:
  # Airflow Worker Service
  airflow-worker:
    <<: *airflow-worker
    # Set hostname to WORKER_NAME for better identification in Flower
    hostname: ${WORKER_NAME}
    # Airflow 3.x uses 'airflow celery worker' command
    # --hostname helps identify workers in monitoring tools (Flower)
    # [PRODUCTION-NOTE] Important for distinguishing multiple workers
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    networks:
      - worker-network
    # [PRODUCTION-CHANGE] Uncomment extra_hosts when deploying to separate servers
    # extra_hosts:
    #   - "main-server:${MAIN_SERVER_IP}"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Optional: Worker-specific Flower for monitoring
  flower-worker:
    <<: *airflow-worker
    command: celery flower --broker=${REDIS_URL:-redis://:${REDIS_PASSWORD:-}@${MAIN_SERVER_IP}:${REDIS_PORT:-6379}/0}
    profiles:
      - flower
    ports:
      - "${FLOWER_PORT:-5555}:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    networks:
      - worker-network
    extra_hosts:
      - "main-server:${MAIN_SERVER_IP}"

  # Optional: DAG Sync Service (using Git)
  dag-sync:
    image: alpine/git:latest
    profiles:
      - git-sync
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags:/dags
    environment:
      GIT_REPO: ${DAG_GIT_REPO}
      GIT_BRANCH: ${DAG_GIT_BRANCH:-main}
      GIT_SYNC_INTERVAL: ${DAG_SYNC_INTERVAL:-60}
    entrypoint: /bin/sh
    command:
      - -c
      - |
        while true; do
          echo "Syncing DAGs from git repository..."
          if [ ! -d /dags/.git ]; then
            git clone --branch $${GIT_BRANCH} $${GIT_REPO} /tmp/repo
            cp -r /tmp/repo/* /dags/
            rm -rf /tmp/repo
          else
            cd /dags && git pull origin $${GIT_BRANCH}
          fi
          echo "DAG sync complete. Waiting $${GIT_SYNC_INTERVAL} seconds..."
          sleep $${GIT_SYNC_INTERVAL}
        done
    restart: always
    networks:
      - worker-network

  # Optional: Log Shipper (Filebeat)
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    profiles:
      - logging
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs:ro
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      ELASTICSEARCH_HOSTS: ${ELASTICSEARCH_HOSTS:-http://${MAIN_SERVER_IP}:9200}
      WORKER_ID: ${WORKER_NAME:-worker-1}
    user: root
    restart: always
    networks:
      - worker-network
    extra_hosts:
      - "elasticsearch:${MAIN_SERVER_IP}"

  # Optional: Monitoring Agent (Node Exporter for Prometheus)
  node-exporter:
    image: prom/node-exporter:latest
    profiles:
      - monitoring
    ports:
      - "${NODE_EXPORTER_PORT:-9100}:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+)($$|/)'
    restart: always
    networks:
      - worker-network

  # Optional: Container Monitoring (cAdvisor)
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    profiles:
      - monitoring
    ports:
      - "${CADVISOR_PORT:-8080}:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    privileged: true
    restart: always
    networks:
      - worker-network

networks:
  worker-network:
    # Network configuration is defined in override files:
    # - DEV:  docker-compose_worker.dev.yaml  (external network, same machine)
    # - PROD: docker-compose_worker.prod.yaml (bridge network, separate machines)
    driver: bridge