# Main Server Environment Configuration
# This file should be placed on the main server running Airflow control plane components

# ============================================
# Basic Airflow Configuration
# ============================================
AIRFLOW_UID=50000
AIRFLOW_IMAGE_NAME=apache/airflow:3.1.3
AIRFLOW_PROJ_DIR=.

# ============================================
# Database Configuration
# ============================================
# IMPORTANT: Change these passwords in production!
POSTGRES_PASSWORD=my_airflow_db_password
POSTGRES_PORT=5432

# ============================================
# Redis Configuration
# ============================================
# IMPORTANT: Set a strong Redis password for production!
REDIS_PASSWORD=my_redis_password
REDIS_PORT=6379

# ============================================
# Security Configuration
# ============================================
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# IMPORTANT: This must be the same on all workers!
FERNET_KEY=your_fernet_key_here_must_be_same_on_all_nodes

# ============================================
# Airflow Web UI Admin Credentials
# ============================================
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=my_admin_password

# ============================================
# API Server Configuration
# ============================================
API_SERVER_PORT=8080

# ============================================
# Scheduler Configuration
# ============================================
SCHEDULER_MAX_THREADS=4
SCHEDULER_PARSING_PROCESSES=2

# ============================================
# Optional: Flower Configuration
# ============================================
FLOWER_PORT=5555

# ============================================
# Optional: Remote Logging Configuration
# ============================================
# Set to true to enable remote logging
REMOTE_LOGGING=false

# For S3 logging
# REMOTE_LOG_FOLDER=s3://your-bucket/airflow-logs
# REMOTE_LOG_CONN_ID=aws_default
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# AWS_DEFAULT_REGION=us-east-1

# For Elasticsearch logging
# REMOTE_LOG_FOLDER=http://elasticsearch:9200
# REMOTE_LOG_CONN_ID=elasticsearch_default

# ============================================
# Optional: Load Example DAGs
# ============================================
LOAD_EXAMPLES=false

# ============================================
# Optional: Additional Python Packages
# ============================================
# _PIP_ADDITIONAL_REQUIREMENTS=boto3 pandas numpy

# ============================================
# Network Configuration
# ============================================
# If you're using specific network interfaces, specify here
# BIND_INTERFACE=0.0.0.0

# ============================================
# Monitoring Configuration (Optional)
# ============================================
# Prometheus metrics
# AIRFLOW__METRICS__STATSD_ON=true
# AIRFLOW__METRICS__STATSD_HOST=statsd-exporter
# AIRFLOW__METRICS__STATSD_PORT=9125
# AIRFLOW__METRICS__STATSD_PREFIX=airflow

# ============================================
# Email Configuration (Optional)
# ============================================
# AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp
# AIRFLOW__EMAIL__EMAIL_CONN_ID=smtp_default
# AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
# AIRFLOW__SMTP__SMTP_STARTTLS=True
# AIRFLOW__SMTP__SMTP_SSL=False
# AIRFLOW__SMTP__SMTP_USER=your_email@gmail.com
# AIRFLOW__SMTP__SMTP_PASSWORD=your_password
# AIRFLOW__SMTP__SMTP_PORT=587
# AIRFLOW__SMTP__SMTP_MAIL_FROM=your_email@gmail.com