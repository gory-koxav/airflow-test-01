# ============================================
# Base Environment Configuration
# Common settings shared between dev and prod
# ============================================

# Basic Airflow Configuration
AIRFLOW_UID=1000
AIRFLOW_IMAGE_NAME=apache/airflow:3.1.3
AIRFLOW_PROJ_DIR=.

# Security Configuration
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# IMPORTANT: This must be the same on all nodes (main + workers)!
FERNET_KEY=your_fernet_key_here_must_be_same_on_all_nodes

# Database Credentials (same for all environments)
POSTGRES_PASSWORD=my_airflow_db_password

# Redis Credentials (same for all environments)
REDIS_PASSWORD=my_redis_password

# Airflow Web UI Admin Credentials
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=my_admin_password

# Scheduler Configuration
SCHEDULER_MAX_THREADS=4
SCHEDULER_PARSING_PROCESSES=2

# Optional: Load Example DAGs
LOAD_EXAMPLES=false

# DAG Activation on Creation
# false = DAGs are active immediately when created
# true = DAGs are paused when created (must manually activate)
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=false

# ============================================
# Logging Configuration
# ============================================
# Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
AIRFLOW__LOGGING__LOGGING_LEVEL=DEBUG

# Optional: Remote Logging Configuration
REMOTE_LOGGING=false
# REMOTE_LOG_FOLDER=s3://your-bucket/airflow-logs
# REMOTE_LOG_CONN_ID=aws_default

# ============================================
# Additional Python Requirements
# ============================================
# NOTE: Python packages are now installed via custom Dockerfiles:
#   - Dockerfile.observer: ML packages for Observer workers
#   - Dockerfile.processor: Lightweight packages for Processor workers
# _PIP_ADDITIONAL_REQUIREMENTS is no longer used for workers.
# Keep empty or remove this line.
_PIP_ADDITIONAL_REQUIREMENTS=

# ============================================
# Shared Storage Configuration
# ============================================
# Container internal path (same for dev and prod)
# Code should always use /opt/airflow/data
AIRFLOW_DATA_PATH=/opt/airflow/data

# Data subdirectories
OBSERVATION_DATA_PATH=/opt/airflow/data/observation
FUSION_DATA_PATH=/opt/airflow/data/fusion
TRACKING_DATA_PATH=/opt/airflow/data/tracking
