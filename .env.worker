# Worker Server Environment Configuration
# This file should be placed on each worker server

# ============================================
# Basic Airflow Configuration
# ============================================
AIRFLOW_UID=50000
AIRFLOW_IMAGE_NAME=apache/airflow:3.1.3
AIRFLOW_PROJ_DIR=.

# ============================================
# Main Server Connection Configuration
# ============================================
# IMPORTANT: Set the IP address or hostname of your main server
# [PRODUCTION-CHANGE] Replace ALL connection settings below when deploying to production
#
# Development (same machine - using Docker network):
#   - POSTGRES_HOST=postgres (container hostname)
#   - REDIS_HOST=redis (container hostname)
#
# Production (separate servers):
#   - POSTGRES_HOST=${MAIN_SERVER_IP} (actual IP like 10.0.0.1)
#   - REDIS_HOST=${MAIN_SERVER_IP} (actual IP like 10.0.0.1)
MAIN_SERVER_IP=10.0.0.1  # [PRODUCTION-CHANGE] Set your main server's actual IP address
POSTGRES_HOST=postgres   # [PRODUCTION-CHANGE] Change to ${MAIN_SERVER_IP} in production
REDIS_HOST=redis         # [PRODUCTION-CHANGE] Change to ${MAIN_SERVER_IP} in production

# ============================================
# Database Connection
# ============================================
# Must match the password set on the main server
POSTGRES_PASSWORD=my_airflow_db_password
POSTGRES_PORT=5432

# ============================================
# Redis Connection
# ============================================
# Must match the password set on the main server
REDIS_PASSWORD=my_redis_password
REDIS_PORT=6379

# Alternative: Full Redis URL (if needed)
# REDIS_URL=redis://:redis_password_change_me@10.0.0.1:6379/0

# ============================================
# Security Configuration
# ============================================
# IMPORTANT: Must be exactly the same as on the main server!
FERNET_KEY=your_fernet_key_here_must_be_same_on_all_nodes

# ============================================
# Worker Identification
# ============================================
# Unique name for this worker (change for each worker)
# [PRODUCTION-CHANGE] Use unique names for each worker in production (e.g., worker-prod-01, worker-prod-02)
WORKER_NAME=worker-dev-local
WORKER_CLASS=sync

# ============================================
# Worker Performance Configuration
# ============================================
# Number of concurrent tasks this worker can run
WORKER_CONCURRENCY=8

# Autoscaling: max,min number of workers
WORKER_AUTOSCALE=16,8

# Maximum number of tasks a worker will process before being recycled
WORKER_MAX_TASKS_PER_CHILD=100

# Number of tasks to prefetch at a time
WORKER_PREFETCH_MULTIPLIER=1

# ============================================
# Resource Limits
# ============================================
# CPU limits and reservations
WORKER_CPU_LIMIT=4
WORKER_CPU_RESERVATION=2

# Memory limits and reservations
WORKER_MEMORY_LIMIT=8G
WORKER_MEMORY_RESERVATION=4G

# ============================================
# Optional: Remote Logging Configuration
# ============================================
# Should match the main server's configuration
REMOTE_LOGGING=false

# For S3 logging
# REMOTE_LOG_FOLDER=s3://your-bucket/airflow-logs
# REMOTE_LOG_CONN_ID=aws_default
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# AWS_DEFAULT_REGION=us-east-1

# For Elasticsearch logging
# REMOTE_LOG_FOLDER=http://elasticsearch:9200
# REMOTE_LOG_CONN_ID=elasticsearch_default

# ============================================
# Optional: DAG Synchronization (Git)
# ============================================
# Enable git-sync profile to use these
# DAG_GIT_REPO=https://github.com/your-org/airflow-dags.git
# DAG_GIT_BRANCH=main
# DAG_SYNC_INTERVAL=60

# For private repositories
# GIT_USERNAME=your_username
# GIT_PASSWORD=your_token

# ============================================
# Optional: Additional Python Packages
# ============================================
# _PIP_ADDITIONAL_REQUIREMENTS=boto3 pandas numpy

# ============================================
# Optional: Monitoring Configuration
# ============================================
# Elasticsearch for log shipping (if using filebeat profile)
# ELASTICSEARCH_HOSTS=http://10.0.0.1:9200

# Node exporter port (if using monitoring profile)
NODE_EXPORTER_PORT=9100

# cAdvisor port (if using monitoring profile)
CADVISOR_PORT=8080

# Flower port (if using flower profile)
FLOWER_PORT=5555

# ============================================
# Network Configuration
# ============================================
# Add custom DNS servers if needed
# DNS_SERVER_1=8.8.8.8
# DNS_SERVER_2=8.8.4.4

# ============================================
# Proxy Configuration (if behind proxy)
# ============================================
# HTTP_PROXY=http://proxy.example.com:8080
# HTTPS_PROXY=http://proxy.example.com:8080
# NO_PROXY=localhost,127.0.0.1,.example.com

# ============================================
# Custom Configuration
# ============================================
# Add any custom environment variables needed by your DAGs below